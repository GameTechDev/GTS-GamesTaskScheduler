Terminology {#dev_guide_prog_mod_terminology}
-----------------

This section covers basic terminology around task scheduling that will be used throughout the documentation.

# Terminologogy

## Fundumental Definitions

<table style="width:100%">
  <tr>
    <td><b>Function:</b> An ordered set of instructions.<img src="Function.svg" style="padding-left:10px;vertical-align:center;"></img></td>
  </tr>
  <tr>
    <td><b>Data Environment:</b> A collection of variables and associated memory addresses.<img src="DataEnv.svg" style="padding-left:10px;vertical-align:center;"></img></td>
  </tr>
  <tr>
    <td><b>Task:</b> An object that encapsulates a function its associated data.<img src="Task.svg" style="padding-left:10px;vertical-align:center;"></img></td>
  </tr>
  <tr>
    <td><b>Execution Entity (EE):</b> A construct that processes a Task. (SW Thread, HW Thread, etc.)<img src="ExecutionEntity.svg" style="padding-left:10px;vertical-align:center;"></img></td>
  </tr>
  <tr>
    <td><b>Potential (or Logical) Parallelism:</b> The concept that a set of Tasks can be executed independently
    by multiple EEs, but only if the EEs are available. (SW Thread, HW Thread, etc.)</td>
    <td></td>
  </tr>
</table>

## Task Graph Model

**Task Graph:** A directed acyclic graph ***D=(N,E)*** that conceptually models the dependency structure of a set of tasks, where
<table style="width:100%">
  <tr>
    <td><ul><li>each <b>Node</b> <b><i>N</i></b> is a task. <img src="Node.svg" style="padding-left:10px;vertical-align:center;"> </li></ul></td>
  </tr>
  <tr>
    <td><ul><li>each <b>edge</b> <b><i>E<sub>i,j</sub></i></b> is a happens-before constraint between nodes <b><i>N<sub>i</sub></i></b> and <b><i>N<sub>j</sub></i></b>.</li></ul></td>
  </tr>
  <tr>
    <td style="padding-left:20%;"><img src="Edge.svg" style="vertical-align:center;"></img></td>
  </tr>
</table>

**Fork:** The splitting of a task into two or more tasks that can be executed independently.

**Join:** The merging of two or more tasks.

<div style="padding-left:20%">
    <img src="ForkJoin.svg" style="vertical-align:center;">
</div>

**Nested Parallelism:** The act of recursively forking tasks to increase parallelism. That is, during execution
each task forks one or more tasksuntil it reaches some base case. Starting with a single task, this produces a tree-like
task graph.

**Strict Task Graph:** A class of task graph, where each forked task is either joined back into ancestor Task or
joined into a continuation Task gererated by an ancestor.

<table style="width:100%">
  <tr>
    <td><img src="StrictDag.svg" style="vertical-align:center;"></img></td>
    <td><img src="StrictDagContinuations.svg" style="vertical-align:center;"></img></td>
  </tr>
</table>

**Fully-strict Task Graph:** A class of task graph, where each forked task is either joined back into its parent or
joined into a continuation Task generated by its parent.

<table style="width:100%">
  <tr>
    <td><img src="FullyStrictDag.svg" style="vertical-align:center;"></img></td>
    <td><img src="FullyStrictDagContinuations.svg" style="vertical-align:center;"></img></td>
  </tr>
</table>

## Scheduling

**Schedule:** The mapping of all nodes ***N*** in a task graph to EEs.

**Execution:** The processing of a schedule by the EEs it‚Äôs mapped to.

**Scheduler:** The construct that builds a schedule and executes it.

<div style="padding-left:10%">
    <img src="Scheduling.svg" style="vertical-align:center;">
</div>

**Offline Scheduler**: A scheduler that generates a schedule before execution.

**Online Scheduler**: A scheduler that discovers a schedule during execution.

### Online Scheduling Categories {#dev_guide_online_sched_cat}

**Static:** At execution time, the task graph topology, task execution times, and the task data transfer times are known.

**Weakly Dynamic:** At execution time, only the task graph topology is known.

**Strongly Dynamic:** At execution time, the task graph topology, the task execution times, the task data transfer times are unknown.

### Greedy Scheduling

A **greedy scheduler** is a scheduler in which no EE are idle if there are more tasks it can execute. The section below looks at several
scheduling strategies that can be used to implement greedy schedulers.

#### Greedy Scheduling Strategies

**Centralization:** Centralization is a strategy where all tasks are stored in a global queue. Tasks are added and
removed FIFO giving a global ordering to task execution.

<div style="padding-left:80px">
    <img src="Centralization.svg" style="vertical-align:center;">
</div>

**Work-distribution:** Work-distribution is a strategy where an EE that produces work distributes tasks to all
other EEs. Tasks are distributed with a heustric that attempts to balance the load, and each EE recieves tasks FIFO via a SPSC
queue.

<div style="padding-left:100px">
    <img src="WorkDistribution.svg" style="vertical-align:center;">
</div>

**Work-sharing:** Work-sharing is a strategy where each EE works locally from its own task collection until it
runs out of tasks, at which point it requests a task from anonther EE. Task are generally stored in deques where they are pushed
and popped LIFO by the owning EE and requests are serviced FIFO. With small enough task sizes, work-sharing automatically load-balances.

<div style="padding-left:140px">
    <img src="WorkSharing.svg" style="vertical-align:center;">
</div>

**Work-stealing:** Work-stealing is a strategy where each EE works locally from its own task collection until it
runs out of tasks, at which point it tries to steal a task from anonther EE. Task are generally stored in deques where they are pushed
and popped LIFO by the owning EE and stolen FIFO from a thief EE. With small enough task sizes, work-stealing automatically load-balances.

<div style="padding-left:140px">
    <img src="WorkStealing.svg" style="vertical-align:center;">
</div>

# Analysis

## Work/Span Model

<table style="width:100%">
  <tr>
    <td><b>Running Time:</b> Denoted <b><i>T<sub>p</sub></i></b>, it is execution time of a program on <b><i>p</i></b> processors.</td>
    <td rowspan="3"><img src="WorkSpan.svg" style="vertical-align:center;"></img></td>
  </tr>
  <tr>
    <td><b>Work:</b> Denoted <b><i>T<sub>1</sub></i></b>, it is the total number of primitive operations performed by
        a single processor during the execution of a program.</td>
  </tr>
  <tr>
    <td><b>Span:</b> Denoted <b><i>T<sub>&infin;</sub></i></b>, it is length of the longest set of operations that
        must be performed serially. It determines the fastest possible execution time idealized by executing a program on
        an infinite number of processors.</td>
  </tr>
</table>

**Work Law:** The time to execute the work on ùëù processors is bounded below by the time to execute on one processor
evenly divided by ***p***. ***T<sub>p</sub> &ge; T<sub>1</sub>/p***.

**Span Law:** The time to execute the work on ***p*** processors is bounded below by the span. ***T<sub>p</sub> &ge; T<sub>&infin;</sub>***.

**Speedup:** ***T<sub>1</sub>/T<sub>p</sub>***. The total work divided by the running time.

**Linear speedup:** ***T<sub>1</sub>/T<sub>p</sub> = p***.

**Parallelism:** ***T<sub>1</sub>/T<sub>p</sub>***. It is the average amount of work per step along the span.

### Combining Graphs

<table style="width:100%">
  <tr>
    <td><img src="Series.svg" style="display:block;margin:auto;vertical-align:center;"></img></td>
    <td><img src="Parallel.svg" style="display:block;margin:auto;vertical-align:center;"></img></td>
  </tr>
    <tr>
    <td><img src="SeriesEq.svg" style="display:block;margin:auto;vertical-align:center;"></img></td>
    <td><img src="ParallelEq.svg" style="display:block;margin:auto;vertical-align:center;"></img></td>
  </tr>
  <tr>
    <td style="text-align:center; padding-top:10px;"><b>Series</b></td>
    <td style="text-align:center; padding-top:10px;"><b>Parallel</b></td>
  </tr>
</table>

# References

1. 1024cores. 2020. *Task Scheduling Strategies.* [ONLINE] Available at: http://www.1024cores.net/home/scalable-architecture/task-scheduling-strategies. [Accessed 14 May 2020].
2. Agrawal, Kunal; Leiserson, Charles E.; Sukha, Jim; *Executing Dynamic Task Graphs Using Work-Stealing.* [ONLINE] Available at: https://www.cse.wustl.edu/~kunal/resources/Papers/nabbit.pdf. [Accessed 14 May 2020].
3. Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2009). *Introduction to Algorithms (3rd ed.).* MIT Press and McGraw-Hill. pp. 779‚Äì784. ISBN 0262033844.
4. wikipedia. 2020. *Analysis of parallel algorithms.* [ONLINE] Available at: https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms. [Accessed 14 May 2020].

<br>
<br>
<br>
<br>























































